{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "- Neural networks account for interactions really well\n",
    "- Deep learning uses especially powerful neural networks\n",
    "\t- Text\n",
    "\t- Images\n",
    "\t- Videos\n",
    "\t- Audio\n",
    "\t- Source code\n",
    "\n",
    "## Forward propagation\n",
    "\n",
    "- Multiply-add process\n",
    "- Dot product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([2, 3])\n",
    "\n",
    "weights = {'node_0': np.array([1, 1]), 'node_1': np.array([-1, 1]), 'output': np.array([2, -1])}\n",
    "\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "- Applied to node inputs to produce node output\n",
    "- ReLU (Rectified Linear Activation)\n",
    "\n",
    "$$\n",
    "\\textrm{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "0, & x < 0 \\\\\n",
    "x, & x \\geq 0\n",
    "\\end{cases}\n",
    "= \\textrm{max}(0, x)\n",
    "$$\n",
    "\n",
    "~~~\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)\n",
    "~~~\n",
    "\n",
    "## Representation learning\n",
    "\n",
    "- Deep networks internally build representations of patterns in the data\n",
    "- Partially replace the need for feature engineering\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data\n",
    "\n",
    "## Deep learning\n",
    "\n",
    "- Modeler doesn't need to specify the interactions\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions\n",
    "\n",
    "## Loss function\n",
    "\n",
    "- Aggregate errors in predictions from many data points into single number\n",
    "- Measure of model's predictive performance\n",
    "\n",
    "- Lower loss function value means a better model\n",
    "- Goal: Find the weights that give the lowest value for the loss function\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "- Start at random point\n",
    "- Until you are somewhere flat:\n",
    "\t- Find the slope\n",
    "\t- Take a step downhill\n",
    "\n",
    "- If the slope is positive:\n",
    "\t- Going opposit ethe slope means moving to lower numbers\n",
    "\t- Subtract the slope from the current value\n",
    "\t- Too big a step might us astray\n",
    "\n",
    "- Solution: learning rate\n",
    "\t- Update each weight by subtracting `learning_rate * slope`\n",
    "\n",
    "## Slope calculation example\n",
    "\n",
    "- To calculate the slope for a weight, need to multiply:\n",
    "\t- Slope of the loss function with respect to (w.r.t.) value at the node we feed into\n",
    "\t- The value of the node that feeds into our weight\n",
    "\t- Slope of the activation function w.r.t. value we feed into\n",
    "\n",
    "~~~\n",
    "\n",
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (input_data * weights_updated).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)\n",
    "~~~\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "- Allows gradient descent to update all weights in NN (by getting gradients for all weights)\n",
    "- Comes from chain rule of Calculus\n",
    "\n",
    "## Backpropagation process\n",
    "\n",
    "- Trying to estimate the slope of the loss function w.r.t. each weight\n",
    "- Go back one layer at a time\n",
    "- Gradients for weight is product of:\n",
    "\t1. Node value feeding into that weight\n",
    "\t2. Slope of loss function w.r.t. node it feeds into\n",
    "\t3. Slope of activation function at the node it feeds into\n",
    "\n",
    "- Need to also keep track of the slopes of the loss function w.r.t. node values\n",
    "- Slope of node value are the sum of the slopes for all weights that come out of them\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "- It is common to calculate slopes on only a subset of the data ('batch')\n",
    "- Use a different batch of data to calculate the next update\n",
    "- Start over from the beginning once all data is used\n",
    "- Each time through the training data is called an epoch\n",
    "- When slopes are calculated one one btach at a time: stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `keras` model building steps\n",
    "\n",
    "1. Specify architecture\n",
    "2. Compile\n",
    "3. Fit\n",
    "4. Predict\n",
    "\n",
    "## Model specification\n",
    "\n",
    "~~~\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',')\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (ncols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "~~~\n",
    "\n",
    "## Why you need to compile your model\n",
    "\n",
    "- Specify the optimizer\n",
    "\t- Controls the learning rate\n",
    "\t- 'Adam' is usually a good choice\n",
    "- Loss function\n",
    "\t- `mean_squared_error`: common for regression\n",
    "\n",
    "~~~\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "~~~\n",
    "\n",
    "## Fitting a model\n",
    "\n",
    "- Applying backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fitting can ease optimization\n",
    "\n",
    "~~~\n",
    "model.fit(predictors, target)\n",
    "~~~\n",
    "\n",
    "## Classification\n",
    "\n",
    "- `categorical_crossentropy` loss function\n",
    "\t- Similar to log loss: lower is better\n",
    "- Add `metrics=['accuracy']` to compile step for easy-to-understand diagnostics\n",
    "- Output layer has separate node for each possible outcome and uses 'softmax' activation\n",
    "\n",
    "~~~\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "\n",
    "predictors = data.drop(['shot_result'], axis=1).as_matrix()\n",
    "\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "~~~\n",
    "\n",
    "## Using models\n",
    "\n",
    "- Save\n",
    "- Reload\n",
    "- Make predictions\n",
    "\n",
    "~~~\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('model_file.h5')\n",
    "\n",
    "my_model = load_model('my_model.h5')\n",
    "\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "\n",
    "probability_true = predictions[:,1]\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "~~~\n",
    "def get_new_model(input_shape = input_shape):\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\treturn model\n",
    "\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "for lr in lr_to_test:\n",
    "\tmodel = get_new_model()\n",
    "\n",
    "\tmy_optimizer = SGD(lr=lr)\n",
    "\n",
    "\tmodel.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "~~~\n",
    "\n",
    "## The dying neuron problem\n",
    "\n",
    "- Once a node starts always getting negative inputs\n",
    "\t- It may continue only getting negative inputs\n",
    "- Contributes nothing to the model\n",
    "\t- 'Dead' neuron\n",
    "\n",
    "## Vanishing gradients\n",
    "\n",
    "- Occurs when many layers have very small slopes (e.g. due to being on flat part of tanh curve)\n",
    "- In deep networks, updates to backprop were close to $0$\n",
    "\n",
    "## Validation in deep learning\n",
    "\n",
    "- Commonly use validation split rather than cross-validation\n",
    "- Deep learning widely used on large datasets\n",
    "- Single validation score is based on large amount of data, and is reliable\n",
    "- Repeated training from cross-validation would take long time\n",
    "\n",
    "~~~\n",
    "model.fit(predictors, target, validation_split=0.3)\n",
    "~~~\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "~~~\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "# patience: num. of epochs without improvement\n",
    "\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks = [early_stopping_monitor])\n",
    "~~~\n",
    "\n",
    "## Workflow for optimizing model capacity\n",
    "\n",
    "- Start with a small network\n",
    "- Get the validation score\n",
    "- Keep increasing capacity until validation score is no longer improving\n",
    "\n",
    "<img src='./IMAGES/sequential-experiments.PNG'>\n",
    "\n",
    "## Recognizing handwritten digits\n",
    "\n",
    "- MNIST dataset\n",
    "- $28 \\times 28$ grid flattened to $784$ values for each image\n",
    "- Value in each part of array denotes darkness of that pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./DATASETS/mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,0]\n",
    "X = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 17:40:44.061921  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0906 17:40:44.084861  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0906 17:40:44.087816  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0906 17:40:44.120166  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0906 17:40:44.151430  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0906 17:40:44.245176  7620 deprecation.py:323] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0906 17:40:44.292002  7620 deprecation_wrapper.py:119] From C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 436us/step - loss: 11.9196 - acc: 0.2507 - val_loss: 11.1853 - val_acc: 0.3033\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 71us/step - loss: 11.3466 - acc: 0.2929 - val_loss: 11.0812 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 79us/step - loss: 10.4002 - acc: 0.3486 - val_loss: 10.3011 - val_acc: 0.3550\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 71us/step - loss: 10.0402 - acc: 0.3721 - val_loss: 10.6183 - val_acc: 0.3383\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 68us/step - loss: 10.0548 - acc: 0.3721 - val_loss: 10.1369 - val_acc: 0.3700\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 67us/step - loss: 9.6243 - acc: 0.3986 - val_loss: 9.8589 - val_acc: 0.3883\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 70us/step - loss: 9.4974 - acc: 0.4079 - val_loss: 9.7577 - val_acc: 0.3917\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 88us/step - loss: 9.1738 - acc: 0.4229 - val_loss: 8.4979 - val_acc: 0.4667\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 63us/step - loss: 7.7556 - acc: 0.5114 - val_loss: 7.7132 - val_acc: 0.5167\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 89us/step - loss: 7.1780 - acc: 0.5514 - val_loss: 7.3401 - val_acc: 0.5383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1571fd06e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, to_categorical(y), epochs=10, validation_split=0.3, callbacks=[EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
