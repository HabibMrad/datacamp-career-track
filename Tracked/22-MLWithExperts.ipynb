{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the challenge\n",
    "\n",
    "- Learn from the expert who won DrivenData's challenge\n",
    "\t- NLP\n",
    "\t- Feature engineering\n",
    "\t- Efficiency boosting hashing tricks\n",
    "- Use data to have a social impact\n",
    "\n",
    "- Budgets for schools are huge, complex, and not standardized\n",
    "\t- Hundreds of hours each year are spent manually labelling\n",
    "- <u>Goal</u>: Build a ML algorithm that can automate the process.\n",
    "\n",
    "- Budget data:\n",
    "\t- Line-item: 'Algebra books for 8th grade students'\n",
    "\t- Labels: 'Textbooks', 'Math', 'Middle School'\n",
    "- This is a supervised learning algorithm\n",
    "\n",
    "## Load and preview the data\n",
    "\n",
    "~~~\n",
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.read_csv('sample_data.csv')\n",
    "~~~\n",
    "\n",
    "## Encode labels as categories\n",
    "\n",
    "- ML algorithms work on numbers, not strings\n",
    "\t- Need a numeric representation of these strings\n",
    "- Strings can be slow compared to numbers\n",
    "- In pandas, 'category' dtype encodes categorical data numerically\n",
    "\t- Can speed up code\n",
    "\n",
    "~~~\n",
    "sample_df.label = sample_df.label.astype('category')\n",
    "~~~\n",
    "\n",
    "- Dummy variable encoding\n",
    "\t- Also called a 'binary indicator' representation\n",
    "\n",
    "~~~\n",
    "dummies = pd.get_dummies(sample_df[['label']], prefix_sep='_')\n",
    "~~~\n",
    "\n",
    "## Encode labels as categories\n",
    "\n",
    "~~~\n",
    "categorize_label = lambda x: x.astype(category)\n",
    "\n",
    "sample_df.label = sample_df[['label']].apply(categorize_label, axis=0)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we measure success?\n",
    "\n",
    "- Accuracy can be misleading when classes are imbalanced\n",
    "- Metric used in this problem: log loss\n",
    "\t- It is a loss function\n",
    "\t- Measure of error\n",
    "\t- Want to minimize the error (unlike accuracy)\n",
    "\n",
    "## Log loss binary classification\n",
    "\n",
    "- Log loss for **binary** classification\n",
    "\t- Actual value: $y = \\{1 = \\textrm{ yes }, 0 = \\textrm{ no }\\}$\n",
    "\t- Prediction (probability that the value is $1$): $p$\n",
    "\n",
    "$$\n",
    "\\textrm{ logloss } = -\\displaystyle\\frac{1}{N} \\displaystyle\\sum_{i=1}^{N} = y_i \\log p_i + (1-y_i) \\log (1-p_i)\n",
    "$$\n",
    "\n",
    "## Computing logloss\n",
    "\n",
    "logloss.py\n",
    "~~~\n",
    "import numpy as np\n",
    "\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "\tpredicted = np.clip(predicted, eps, 1-eps)\n",
    "\tloss = -1 * np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "\n",
    "\treturn loss\n",
    "~~~\n",
    "\n",
    "## It's time to build a model\n",
    "\n",
    "- Train basic model on numeric data only\n",
    "\t- Want to go from raw data to predictions quickly\n",
    "- Multi-class logistic regression\n",
    "\t- Train classifier on each label separately and use those to predict\n",
    "\n",
    "## Splitting the multi-class dataset\n",
    "\n",
    "- Recall: train-test split\n",
    "\t- Will not work here\n",
    "\t- may end up with labels in test set that never appear in training set\n",
    "- Solution: `StratifiedShuffleSplit`\n",
    "\t- Only works with a single target variable\n",
    "\t- We have many target variables\n",
    "\t- `multilabel_train_test_split()`\n",
    "\n",
    "~~~\n",
    "data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size=0.2, seed=123)\n",
    "~~~\n",
    "\n",
    "## Training the model\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "~~~\n",
    "\n",
    "- `OneVsRestClassifier`:\n",
    "\t- treats each column of `y` independently\n",
    "\t- fits a separate classifier for each of the columns\n",
    "\n",
    "## Predicting on houldout data\n",
    "\n",
    "~~~\n",
    "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
    "\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "predictions = clf.predict_proba(holdout)\n",
    "~~~\n",
    "\n",
    "- if `.predict()` was used instead:\n",
    "\t- output would be $0$ or $1$\n",
    "\t- logloss penalizes being confident and wrong\n",
    "\t\t- worse performance compared to `.predict_proba()`\n",
    "\n",
    "## Submitting your predictions as a csv\n",
    "\n",
    "~~~\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS], prefix_sep='__').columns, index=holdout.index,data=predictions)\n",
    "\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very brief introduction fo NLP\n",
    "\n",
    "- Tokenization\n",
    "\t- Splitting a string into segments\n",
    "\t- Store segments as list\n",
    "\t- Examples:\n",
    "\t\t- Tokenize on whitespace\n",
    "\t\t- Tokenize on whitespace and punctuation\n",
    "\n",
    "- Bag of words representation\n",
    "\t- counts the number of times a particular token appears\n",
    "\t- this approach discards information about the word order\n",
    "\t\t- 'Red, not blue' is the same as 'blue, not red'\n",
    "\t- can use n-grams\n",
    "\n",
    "## Representing text numerically\n",
    "\n",
    "- Bag of words\n",
    "\t- simple way to represent text in ML\n",
    "\t- discards information about grammar and word order\n",
    "\t- computes frequency of occurence\n",
    "\n",
    "## Bag of words in Sklearn\n",
    "\n",
    "- `CountVectorizer()`\n",
    "\t- tokenizes all the strings\n",
    "\t- builds a 'vocabulary'\n",
    "\t- counts the occurences of each token in the vocabulary\n",
    "\n",
    "~~~\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "df.Program_Description.fillna('', inplace=True)\n",
    "\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "vec_basic.fit(df.Program_Description)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline workflow\n",
    "\n",
    "- Repeatable way to go from raw data to trained model\n",
    "- Pipeline object takes sequential list of steps\n",
    "\t- Output of one step is input to next step\n",
    "- Each step is a tuple with two elements\n",
    "\t- Name: string\n",
    "\t- Transform: object implementing `.fit()` and `.transform()`\n",
    "- Flexible: a step can itself be another pipeline!\n",
    "\n",
    "- Instantiate simple pipeline with on step\n",
    "\n",
    "~~~\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "pl = Pipeline([('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "~~~\n",
    "\n",
    "- Train an dtest with sample numeric data\n",
    "\n",
    "~~~\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']], pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "~~~\n",
    "\n",
    "- Imputing values\n",
    "\n",
    "~~~\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric','with_missing']], pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "pl = Pipeline([('imp', Imputer()),\n",
    "\t\t('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "\t\t])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "acc = pl.score(X_test, y_test)\n",
    "~~~\n",
    "\n",
    "- Preprocessing text features\n",
    "\n",
    "~~~\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'], pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "pl = Pipeline([('vec', CountVectorizer()),('clf', OneVsRestClassifier())])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "acc = pl.score(X_test, y_test)\n",
    "~~~\n",
    "\n",
    "- Preprocessing multiple dtypes\n",
    "\t- Want to use <u>all</u> available features in one pipeline\n",
    "\t- Problem\n",
    "\t\t- Pipeline steps for numeric and text preprocessing can't follow each other\n",
    "\t- Solution\n",
    "\t\t- `FunctionTransformer()` & `FeatureUnion()`\n",
    "\n",
    "\n",
    "## FunctionTransformer\n",
    "\n",
    "- Turns a Python function into an object that scikit-learn pipeline can understand\n",
    "- Need to write two functions for pipeline preprocessing\n",
    "\t- Take entire DataFrame, return numeric columns\n",
    "\t- Take entire DataFrame, return text columns\n",
    "- Can then preprocess numeric and text data in separate pipelines\n",
    "\n",
    "~~~\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric','with_missing','text']], pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric','with_missing']], validate=False)\n",
    "~~~\n",
    "\n",
    "- Putting it all together\n",
    "\n",
    "~~~\n",
    "numeric_pipeline = Pipeline([('selector', get_numeric_data), ('imputer', Imputer())])\n",
    "\n",
    "text_pipeline = Pipeline([('selector', get_text_data), ('vectorizer', CountVectorizer())])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\t\t\t('union', FeatureUnion([('numeric', numeric_pipeline), ('text', text_pipeline)])),\n",
    "\t\t\t('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "\t\t    ])\n",
    "~~~\n",
    "\n",
    "## Learning from the expert: text preprocessing\n",
    "\n",
    "- NLP tricks for text data\n",
    "\t- Tokenize on punctuation to avoid hyphens, underscores etc.\n",
    "\t- Include unigrams **and** bi-grams in the model to capture important information involving multiple tokens - e.g., 'middle school'\n",
    "\n",
    "- N-grams and tokenization\n",
    "\n",
    "~~~\n",
    "vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "~~~\n",
    "\n",
    "## Learning from the expert: interaction terms\n",
    "\n",
    "- Example\n",
    "\t- 'English teacher for 2nd grade'\n",
    "\t- '2nd grade - budget for English teacher'\n",
    "- Interaction terms mathematically describe when tokens appear together\n",
    "- The math: $\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\times x_2)$\n",
    "- With scikit-learn\n",
    "\n",
    "~~~\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "interaction.fit_transform(x)\n",
    "~~~\n",
    "\n",
    "## Learning from the expert: hashing trick\n",
    "\n",
    "- Adding new features may cause enormous increase in array size\n",
    "- Hashing is a way of increasing memory efficiency\n",
    "- Hash function limits possible outputs, fixing array size\n",
    "\n",
    "~~~\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vec = HashingVectorizer(norm=None, non_negative=True, token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "~~~\n",
    "\n",
    "## The model that won it all\n",
    "\n",
    "- NLP: range of n-grams, punctuation tokenization\n",
    "- Stats: interaction terms\n",
    "- Computation: hashing trick\n",
    "- Model: `LogisticRegression` !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full winning notebook available [here](https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
