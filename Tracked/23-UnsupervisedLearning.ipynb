{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "- Finds patterns in data\n",
    "- Compressing the data using patterns (dimension reduction)\n",
    "\n",
    "## Supervised learning vs unsupervised learning\n",
    "\n",
    "- *Supervised* learning finds patterns for a prediction task\n",
    "\t- E.g.: classify tumors as benign or cancerous (labels)\n",
    "- *Unsupervised* learning finds patterns in data <u>but without a specific prediction task in mind</u>\n",
    "\n",
    "## K-means clustering\n",
    "\n",
    "~~~\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "labels = model.predict(samples)\n",
    "~~~\n",
    "\n",
    "## Cluster labels for new samples\n",
    "\n",
    "- New samples can be assigned to existing clusters\n",
    "- k-means remembers the mean of each cluster (the \"centroids\")\n",
    "- Finds the nearest centroid to each new sample\n",
    "\n",
    "~~~\n",
    "new_labels = model.predict(new_samples)\n",
    "~~~\n",
    "\n",
    "## Cross tabulation with pandas\n",
    "\n",
    "- Clusters vs species is a \"cross-tabulation\"\n",
    "- Use the *pandas* library\n",
    "- Given the `species` of each sample as a list `species`\n",
    "\n",
    "~~~\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "~~~\n",
    "\n",
    "## Measuring clustering quality\n",
    "\n",
    "- Using only samples and their cluster labels\n",
    "- A good clustering has tight clusters\n",
    "- ... and samples in each cluster bunched together\n",
    "\n",
    "## Inertia measures clustering quality\n",
    "\n",
    "- Measures how spread out the clusters are\n",
    "- Distance from each sample to centroid of its cluster\n",
    "- After `.fit()`, available as attribute `inertia_`\n",
    "- k-means attempts to minimize the inertia when choosing clusters\n",
    "\n",
    "## The number of clusters\n",
    "\n",
    "- More clusters means lower inertia\n",
    "- What is the best number of cluster?\n",
    "\n",
    "- A good clustering has tight clusters (low inertia)\n",
    "- ... but **not too many clusters**!\n",
    "- Choose an \"elbow\" in the inertia plot:\n",
    "\t- Where inertia begins to decrease more slowly\n",
    "\n",
    "## `StandardScaler`\n",
    "\n",
    "- In k-means: feature variance = feature influence\n",
    "- `StandardScaler` transforms each feature to have mean $0$ and variance $1$\n",
    "- Features are said to be \"standardized\"\n",
    "\n",
    "~~~\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(samples)\n",
    "\n",
    "samples_scaled = scaler.transform(samples)\n",
    "~~~\n",
    "\n",
    "## Pipelines combine multiple steps\n",
    "\n",
    "~~~\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "from sklearn.pipeline impor make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "\n",
    "pipeline.fit(samples)\n",
    "labels = pipeline.predict(samples)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "- Clusters are contained in one another\n",
    "\n",
    "## Agglomerative hierarchical clustering\n",
    "\n",
    "- Every element begins in a separate cluster\n",
    "- At each step, the two closest clusters are merged\n",
    "- Continue until all countries in a single cluster\n",
    "\n",
    "## The dendrogram of a hierarchical clustering\n",
    "\n",
    "- Read from the bottom up\n",
    "- Vertical lines represent clusters\n",
    "\n",
    "<img src='./IMAGES/dendrogram.PNG'>\n",
    "\n",
    "## With SciPy\n",
    "\n",
    "~~~\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)\n",
    "\n",
    "plt.show()\n",
    "~~~\n",
    "\n",
    "## Intermediate clusterings & height on dendrogram\n",
    "\n",
    "- An intermediate stage in the hierarchical clustering is specified by choosing a height on the dendrogram\n",
    "- The y-axis of the dendrogram encodes the distance between merging clusters\n",
    "\n",
    "## Distance between clusters\n",
    "\n",
    "- Defined by a \"linkage method\"\n",
    "- Specified via method parameters, e.g. `linkage(samples, method='complete')`\n",
    "\t- In 'complete' linkage: distance between clusters is maximum distance between their samples\n",
    "\n",
    "## Extracting cluster labels\n",
    "\n",
    "- Use the `fcluster` method\n",
    "- Returns a NumPy array of cluster labels\n",
    "\n",
    "~~~\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "\n",
    "print(labels)\n",
    "~~~\n",
    "\n",
    "## Aligning cluster labels with country names\n",
    "\n",
    "- Given a list of strings `country_names`:\n",
    "\n",
    "~~~\n",
    "pairs = pd.DataFrame({'labels': labels, 'countries': country_names})\n",
    "\n",
    "print(pairs.sort_values('labels'))\n",
    "~~~\n",
    "\n",
    "## t-SNE for 2-dimensional maps\n",
    "\n",
    "- t-SNE: \"t-distributed stochastic neighbor embedding\"\n",
    "- Maps samples to 2D space (or 3D)\n",
    "- Map approximately preserves nearness of samples\n",
    "\n",
    "~~~\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(learning_rate=100)\n",
    "\n",
    "transformed = model.fit_transform(samples)\n",
    "\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()\n",
    "~~~\n",
    "\n",
    "- **t-SNE has only `.fit_tranform()`**\n",
    "\t- Can't extend the map to include new data samples\n",
    "- Learning rate\n",
    "\t- Choose learning rate for the dataset\n",
    "\t- Wrong choice: points bunch together\n",
    "\t- Try values between 50 and 200\n",
    "- The axes do not have any interpretable meaning\n",
    "\t- They are different every time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "- More efficient storage and computation\n",
    "- Remove less-informative 'noise' features which cause problems for prediction tasks\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- First step: decorrelation\n",
    "- Second step: reduces dimension\n",
    "\n",
    "## PCA aligns data with axes\n",
    "\n",
    "- Rotates data samples to be aligned with axes\n",
    "- Shifts data samples so they have mean 0\n",
    "- No information is lost\n",
    "\n",
    "## PCA in scikit-learn\n",
    "\n",
    "- `.fit()` learns the transformation from given data\n",
    "- `.transform()` applies the learned data\n",
    "- `.transform()` can also be applied to new data\n",
    "\n",
    "~~~\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA()\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "transformed = model.transform(samples)\n",
    "~~~\n",
    "\n",
    "## PCA features\n",
    "\n",
    "- Rows of `transformed` correspond to samples\n",
    "- Columns of `transformed` are the 'PCA features'\n",
    "- Row gives PCA feature values of corresponding sample\n",
    "\n",
    "- *PCA features are no correlated*\n",
    "\n",
    "## Pearson correlation\n",
    "\n",
    "- Measures linear correlation of features\n",
    "- Value between $-1$ and $1$\n",
    "- Value of $0$ means no linear correlation\n",
    "\n",
    "## Principal components\n",
    "\n",
    "- Directions of variance\n",
    "- PCA aligns principal components with the axes\n",
    "- Available as `.components_` attribute of PCA object\n",
    "\n",
    "## Instrinsic dimension of a flight\n",
    "\n",
    "- 2 features: long & lat\n",
    "- Dataset **appears** to be 2-dimensional\n",
    "- But can approximate using one feature: displacement along flight path\n",
    "- Is intrinsically 1-dimensional\n",
    "\n",
    "<img src='./IMAGES/flight-path.PNG'>\n",
    "\n",
    "## Intrinsic dimension\n",
    "\n",
    "- number of features needed to approximate the dataset\n",
    "- essential idea behind dimension reduction\n",
    "\t- What is the most compact representation of the samples?\n",
    "- can be detected with PCA\n",
    "\t- **intrinsic dimension = number of PCA features with significant variance**\n",
    "\n",
    "- Plotting the variances of the PCA features\n",
    "\n",
    "~~~\n",
    "pca = PCA()\n",
    "pca.fit(samples)\n",
    "\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "\n",
    "plt.xticks(features)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "\n",
    "plt.show()\n",
    "~~~\n",
    "***\n",
    "~~~\n",
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "~~~\n",
    "\n",
    "## Dimension reduction with PCA\n",
    "\n",
    "- PCA features are in decreasing order of variance\n",
    "- Assumes the low variance features are 'noise'\n",
    "- ... and high variance features are informative\n",
    "\n",
    "- Specify how many features to keep\n",
    "\t- E.g.: `PCA(n_components=2)`\n",
    "\t- Keeps the first 2 PCA features\n",
    "- Intrinsic dimension is a good choice\n",
    "\n",
    "## Word frequency arrays\n",
    "\n",
    "- Rows represent documents, columns represent words\n",
    "- Entries measure presence of each word in each document\n",
    "- ... measure using 'tf-idf'\n",
    "- Usually give rise to sparse matrices\n",
    "\n",
    "## Sparse arrays and `csr_matrix`\n",
    "\n",
    "- Array is 'sparse': most entries are zero\n",
    "- Can use `scipy.sparse.csr_matrix` instead of NumPy array\n",
    "- `csr_matrix` remembers only the non-zero entries (saves space!)\n",
    "\n",
    "## `TruncatedSVD` and `csr_matrix`\n",
    "\n",
    "- scikit-learn PCA doesn't support `csr_matrix`\n",
    "- Use scikit-learn `TruncatedSVD` instead\n",
    "- Performs same transformation\n",
    "\n",
    "~~~\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "model = TruncatedSVD(n_components=3)\n",
    "\n",
    "model.fit(documents)\n",
    "\n",
    "transformed = model.transform(documents)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)\n",
    "\n",
    "- Dimension reduction technique\n",
    "- NMF models are *interpretable* (unlike PCA)\n",
    "\t- Easy to interprest means easy to explain!\n",
    "- However, all sample features must be non-negative ($/geq 0$)\n",
    "\n",
    "## Interpretable parts\n",
    "\n",
    "- NMF expresses documents as combinations of topics (or 'themes') and images as combinations of patterns\n",
    "\n",
    "## NMF in scikit-learn\n",
    "\n",
    "- Follows `.fit()` / `.transform()` pattern\n",
    "- <u>Must</u> specify the number of components\n",
    "\t- `NMP(n_components=2)`\n",
    "- Works with NumPy arrays and with `csr_matrix`\n",
    "\n",
    "- Example: word-frequency array\n",
    "\t- 4 words, many documents\n",
    "\t- Measure presence of words in each document using 'tf-idf'\n",
    "\t\t- 'tf': frequency of the word\n",
    "\t\t- 'idf': reduces influence of frequent words\n",
    "\n",
    "~~~\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=2)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "nmf_features = model.transform(samples)\n",
    "~~~\n",
    "\n",
    "## NMF components\n",
    "\n",
    "- Dimension of components = dim. of samples\n",
    "- Entries are non-negative\n",
    "\n",
    "## NMF features\n",
    "\n",
    "- NMF features values are non-negative\n",
    "- Can be used to reconstruct the samples\n",
    "- ... combine feature values with components\n",
    "\n",
    "## Reconstruction of a sample\n",
    "\n",
    "- Multiply each NMF feature by the corresponding NMF component and add up\n",
    "- Can also be expressed as a product of matrices\n",
    "\t- This is the \"Matrix Factorization\" in NMF\n",
    "\n",
    "## NMF learns interpretable parts\n",
    "\n",
    "- Word-frequency array articles (tf-idf)\n",
    "\t- 20,000 scientific articles (rows)\n",
    "\t- 800 words (columns)\n",
    "\n",
    "~~~\n",
    "nmf= NMF(n_components=10)\n",
    "\n",
    "nmf.fit(articles)\n",
    "~~~\n",
    "\n",
    "- NMF components are topics\n",
    "- For documents:\n",
    "\t- NMF components (rows) represent topics\n",
    "\t- NMF features combine topics into documents\n",
    "- For images, NMF components are parts of images\n",
    "\n",
    "## Grayscale image\n",
    "\n",
    "- no colors, only shades of gray\n",
    "- measure pixel brightness\n",
    "- represent with value between $0$ and $1$ ($0$ is black)\n",
    "- convert to 2D array\n",
    "\n",
    "- as a flat array: row-by-row, from left to right\n",
    "\n",
    "## Encoding a collection of images\n",
    "\n",
    "- Collection of images of the same size\n",
    "- Encode as 2D array\n",
    "- Each row corresponds to an image\n",
    "- Each column corresponds to a pixel\n",
    "\n",
    "## Visualizing samples\n",
    "\n",
    "~~~\n",
    "bitmap = sample.reshape((2,3))\n",
    "\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.show()\n",
    "~~~\n",
    "\n",
    "## Finding similar articles\n",
    "\n",
    "- Similar articles should have similar topics\n",
    "- Strategy:\n",
    "\t- Apply NMF to the word-frequency array\n",
    "\t- NMF feature values describe the topics\n",
    "\t- ... so similar documents have similar NMF feature values\n",
    "\n",
    "- `articles` is a word frequecy array\n",
    "\n",
    "~~~\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=6)\n",
    "\n",
    "nmf_features = nmf.fit_transform(articles)\n",
    "~~~\n",
    "\n",
    "- Different versions of the same document have some topic *proportions*\n",
    "\t- ... exact feature values may be different!\n",
    "- **But all versions lie on the same line through the origin**!\n",
    "\n",
    "## Cosine similarity\n",
    "\n",
    "- Uses the angle between the lines\n",
    "- Max: $1$, when angle is $0^0$\n",
    "- Higher values mean more similar\n",
    "\n",
    "~~~\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "current_article = norm_features[23,:]\n",
    "\n",
    "similarities = norm_features.dot(current_article)\n",
    "\n",
    "print(similarities)\n",
    "~~~\n",
    "\n",
    "## DataFrames and labels\n",
    "\n",
    "- Titles given as a list `titles`\n",
    "\n",
    "~~~\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "current_article = df.loc['Dog bites man']\n",
    "\n",
    "similarities = df.dot(current_article)\n",
    "\n",
    "print(similarities.nlargest())\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "~~~\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)\n",
    "\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen']\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "print(similarities.nlargest())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
