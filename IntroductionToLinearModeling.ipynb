{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Modeling in Python\n",
    "\n",
    "- Interpolation: model prediction for points in between the data at hand.\n",
    "- Extrapolation: model prediction for values outside the range of the data at hand.\n",
    "\n",
    "#### Object interface\n",
    "\n",
    "~~~\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "options = dict(marker='o', color='blue')\n",
    "\n",
    "line = ax.plot(x, y, **options)\n",
    "\n",
    "_ = ax.set_xlabel('X label')\n",
    "_ = ax.set_ylabel('Y label')\n",
    "\n",
    "plt.show()\n",
    "~~~\n",
    "\n",
    "### Covariance\n",
    "\n",
    "- Measures how two variables vary together.\n",
    "\n",
    "~~~\n",
    "# deviations of two variables\n",
    "dx = x - np.mean(x)\n",
    "dy = y - np.mean(y)\n",
    "\n",
    "deviation_products = dx * dy\n",
    "\n",
    "covariance = np.mean(deviation_products)\n",
    "~~~\n",
    "\n",
    "### Correlation\n",
    "\n",
    "~~~\n",
    "# divide deviations by std. dev.\n",
    "zx = dx/np.std(x)\n",
    "zy = dy/np.std(y)\n",
    "\n",
    "# mean of the normalized deviations\n",
    "correlation = np.mean(zx*zy)\n",
    "~~~\n",
    "\n",
    "### Magnitude vs Direction\n",
    "\n",
    "- Correlation values: -1 to +1\n",
    "- Two parts: magnitude (0 to 1) and sign (+ or -)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor Series\n",
    "\n",
    "- Things to know:\n",
    "\t1. approximate any curve;\n",
    "\t2. polynomial: $ y = a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3 + ... + a_n \\cdot x^n $\n",
    "\t3. often, first order is enough: y = a0 + a1*x\n",
    "\n",
    "### Model\n",
    "\n",
    "$y = a_0 + a_1\\ x$\n",
    "\n",
    "- $a_0$: intercept\n",
    "\t- gives the value of y where the line intercepts the  y-axis (x=0).\n",
    "- $a_1$: slope\n",
    "\t- measure of how sensitive a dependency exists between the two variables ($\\displaystyle\\frac{\\Delta\\ x}{\\Delta\\ y} $).\n",
    "\n",
    "#### Rescaling vs Dependency\n",
    "\n",
    "- Conversion between Celsius and Fahrenheit degrees:\n",
    "\n",
    "~~~\n",
    "slope = (212 - 32)/(100 - 0) # 180/100 = 9/5\n",
    "intercept = 32\n",
    "~~~\n",
    "\n",
    "#### Residuals\n",
    "\n",
    "~~~\n",
    "residuals = y_model - y_data\n",
    "\n",
    "residuals_squared = np.square(residuals)\n",
    "\n",
    "rss = np.sum(residuals_squared)\n",
    "~~~\n",
    "\n",
    "- RSS: residuals squared sum.\n",
    "- Optimization: minimization of RSS.\n",
    "\n",
    "### Least-Squares Optimization\n",
    "\n",
    "- Setting RSS slope := zero, and some calculus, yields:\n",
    "\n",
    "$a_1 = \\displaystyle\\frac{Cov(x,y)}{Var(x)}$\n",
    "\n",
    "and\n",
    "\n",
    "$a_0 = \\bar{y} - a_1 \\cdot \\bar{x}$\n",
    "\n",
    "#### In Numpy\n",
    "\n",
    "~~~\n",
    "x_mean = np.sum(x)/len(x)\n",
    "y_mean = np.sum(y)/len(y)\n",
    "\n",
    "x_dev = x - x_mean\n",
    "y_dev = y - y_mean\n",
    "\n",
    "a1 = np.sum(x_dev * y_dev)/np.sum(x_dev**2)\n",
    "a0 = y_mean - a1*x_mean\n",
    "~~~\n",
    "\n",
    "#### In Scipy\n",
    "\n",
    "~~~\n",
    "from scipy import optimize\n",
    "\n",
    "x_data, y_data = load_data()\n",
    "\n",
    "def model_func(x, a0, a1):\n",
    "\treturn a0 + a1*x\n",
    "\n",
    "param_opt, param_cov = optimize.curve_fit(model_func, x_data, y_data)\n",
    "\n",
    "a0 = param_opt[0]\n",
    "a1 = param_opt[1]\n",
    "~~~\n",
    "\n",
    "#### In Statsmodels\n",
    "\n",
    "~~~\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "x_data, y_data = load_data()\n",
    "\n",
    "df = pd.DataFrame(dict(x_name=x_data,y_name=y_data))\n",
    "\n",
    "model_fit = ols(formula='y_name ~ x_name', data=df).fit()\n",
    "\n",
    "y_model = model_fit.predict(df)\n",
    "x_model = x_data\n",
    "\n",
    "a0 = model_fit.params['Intercept']\n",
    "a1 = model_fit.params['x_name']\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "~~~\n",
    "from sklearn.linear_model import Linear Regression\n",
    "\n",
    "# Initialize a general model\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Load and shape the data\n",
    "x_raw, y_raw = load_data()\n",
    "x_data = x_raw.reshape(len(x_raw),1)\n",
    "y_data = x_raw.reshape(len(y_raw),1)\n",
    "\n",
    "model_fit = model.fit(x_data,y_data)\n",
    "~~~\n",
    "\n",
    "#### Predictions and parameters\n",
    "\n",
    "~~~\n",
    "# Extract the linear model parameters\n",
    "\n",
    "intercept = model.intercept_[0]\n",
    "slope = model.coef_[0,0]\n",
    "\n",
    "# Use the model to make predictions\n",
    "future_x = 2100\n",
    "future_y = model.predict(future_x)\n",
    "~~~\n",
    "\n",
    "### statsmodels\n",
    "\n",
    "~~~\n",
    "x, y = load_data()\n",
    "\n",
    "df = pd.DataFrame(dict(times=x_data,distances=y_data))\n",
    "\n",
    "model_fit = ols(formula='distances ~ times',data=df).fit()\n",
    "~~~\n",
    "\n",
    "### Uncertainty\n",
    "\n",
    "~~~\n",
    "a0 = model_fit.params['Intercept'] # intercept\n",
    "a1 = model_fit.params['times'] # slope\n",
    "\n",
    "e0 = model_fit.bse['Intercept'] # error in intercept\n",
    "e1 = model_fit.bse['times'] # error in slope\n",
    "~~~\n",
    "\n",
    "### Goodness-of-fit\n",
    "\n",
    "#### 3 Different Rs\n",
    "\n",
    "- Building models:\n",
    "\t- **RSS**\n",
    "- Evaluting models:\n",
    "\t- **RMSE**\n",
    "\t- **R-squared**\n",
    "\n",
    "#### RMSE\n",
    "\n",
    "~~~\n",
    "residuals = y_model - y_data\n",
    "rss = np.sum(np.square(residuals))\n",
    "\n",
    "mean_squared_residuals = rss/len(residuals)\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = np.mean(np.square(residuals))\n",
    "\n",
    "# Root Mean Squared Error\n",
    "rmse = np.sqrt(mse) # Std. Deviation (spread) of residuals\n",
    "~~~\n",
    "\n",
    "#### R-squared\n",
    "\n",
    "- Check how much of the variation on the data is due to the linear trend and how much is not.\n",
    "\n",
    "~~~\n",
    "# Deviations\n",
    "deviations = np.mean(y_data) - y_data\n",
    "var = np.sum(np.square(deviations))\n",
    "\n",
    "# Residuals\n",
    "residuals = y_model - y_data\n",
    "rss = np.sum(np.square(residuals))\n",
    "\n",
    "r_squared = 1 - (rss/var)\n",
    "r = correlation(y_data,y_model)\n",
    "~~~\n",
    "\n",
    "#### RMSE vs R-Squared\n",
    "\n",
    "- RMSE: how much variation is residual\n",
    "- R-squared: what fraction of variation is linear\n",
    "\n",
    "When variation due to linear trend is larger than variation due to residuals, the model is better.\n",
    "\n",
    "### Standard Error\n",
    "\n",
    "#### Uncertainty in parameters\n",
    "\n",
    "- Parameter value as center (mean)\n",
    "- Parameter standard error as spread (std. deviation)\n",
    "- Std. Error measures parameter uncertainty\n",
    "\n",
    "#### Computing SE\n",
    "\n",
    "~~~\n",
    "df = pd.DataFrame(dict(times=x_data,distances=y_data))\n",
    "\n",
    "model_fit = ols(formula='distances ~ times',data=df).fit()\n",
    "\n",
    "a1 = model_fit.params['times'] # slope\n",
    "a2 = model_fit.params['Intercept'] # intercept\n",
    "\n",
    "e0 = model_fit.bse['Intercept'] # SE of intercept\n",
    "e1 = model_fit.bse['times'] # SE of slope\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential Statistics Concepts\n",
    "\n",
    "#### Resampling\n",
    "\n",
    "~~~\n",
    "# Resampling as Iteration\n",
    "\n",
    "num_samples = 20\n",
    "for ns in range(num_samples):\n",
    "\tsample = np.random.choice(population,num_pts)\n",
    "\tdistribution_of_means[ns] = sample.mean()\n",
    "\n",
    "# Sample distribution statistics\n",
    "mean_of_means = np.mean(distribution_of_means)\n",
    "stddev_of_means = np.std(distribution_of_means)\n",
    "~~~\n",
    "\n",
    "#### Estimation\n",
    "\n",
    "~~~\n",
    "# Define Gaussian model function\n",
    "def gaussian_model(x, mu, sigma):\n",
    "\tcoeff_part = 1/(np.sqrt(2 * np.pi * sigma**2))\n",
    "\texp_part = np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "\treturn coeff_part * exp_part\n",
    "\n",
    "# Compute the sample statistics\n",
    "mean = np.mean(sample)\n",
    "stdev = np.std(sample)\n",
    "\n",
    "# Model the population using sample statistics\n",
    "population_model = gaussian_model(sample,mean,stdev)\n",
    "~~~\n",
    "\n",
    "#### Likelihood vs Probability\n",
    "\n",
    "- Conditional probability: $P(\\textrm{outcome }A|\\textrm{given }B)$\n",
    "- Probability: $P(\\textrm{data}|\\textrm{model})$\n",
    "\t- If a model is given, we ask what is the **probability** that it outputs a particular data point.\n",
    "- Likelihood: $L(\\textrm{model}|\\textrm{data})$\n",
    "\t- If the data is given, we ask the what is the **likelihood** that a candidate model could output the particular data set we already have.\n",
    "\n",
    "- If we had two candidate models, we'd want to choose the one that has the greatest likelihood to output the given data.\n",
    "\n",
    "#### Computing Likelihood\n",
    "\n",
    "- Likelihood from parameters\n",
    "\n",
    "~~~\n",
    "# Guess parameters\n",
    "mu_guess = np.mean(sample_distances)\n",
    "sigma_guess = np.std(sample_distances)\n",
    "\n",
    "# For each sample point, compute a probability\n",
    "probabilities = np.zeros(len(sample_distances))\n",
    "for n, distance in enumerate(sample_distances):\n",
    "\tprobabilities[n] = gaussian_model(distance,mu_guess,sigma_guess)\n",
    "\n",
    "likelihood = np.product(probs)\n",
    "loglikelihood = np.sum(np.log(probs))\n",
    "~~~\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "~~~\n",
    "# Create an array of mu guesses\n",
    "low_guess = sample_mean - 2*sample_stdev\n",
    "high_guess = sample_mean + 2*sample_stdev\n",
    "mu_guesses = np.linspace(low_guess,high_guess, 101)\n",
    "\n",
    "# Compute the loglikelihood for each guess\n",
    "loglikelihoods = np.zeros(len(mu_guesses)))\n",
    "for n, mu_guess in enumerate(mu_guesses):\n",
    "\tloglikelihood[n] = compute_loglikelihood(sample_distances,mu_guess,sample_stdev)\n",
    "\n",
    "# Find the best guess\n",
    "max_loglikelihood = np.max(loglikelihoods)\n",
    "best_mu = mu_guesses[loglikelihoods == max_loglikelihood]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap resampling\n",
    "\n",
    "~~~\n",
    "# Use sample as model for population\n",
    "population_model = august_daily_highs_for_2017\n",
    "\n",
    "# Simulate repeated data acquisitions by resampling the 'model'\n",
    "for nr in range(num_resamples):\n",
    "\tbootstrap_sample = np.random.choice(population_model,size=resample_size,replace=True)\n",
    "\tbootstrap_means[nr] = np.mean(bootstrap_sample)\n",
    "~~~\n",
    "\n",
    "### Types of Error\n",
    "\n",
    "1. Measurement error\n",
    "\t- broken sensor, worngly recorded measurement.\n",
    "2. Sampling bias\n",
    "\t- temperatures only from August, when days are hottest.\n",
    "3. Random chance\n",
    "\n",
    "#### Null hypotthesis\n",
    "\n",
    "- Question: Is our effect due to a relationship or due to random chance?\n",
    "\n",
    "- Test statistic:\n",
    "\n",
    "~~~\n",
    "# Group into early and late times\n",
    "group_short = sample_distance[times < 5]\n",
    "group_long = sample_distance[times > 5]\n",
    "\n",
    "# Resample distributions\n",
    "resample_short = np.random.choice(group_short,size=500,replace=True)\n",
    "resample_long = np.random.choice(group_long,size=500,replace=True)\n",
    "\n",
    "test_statistic = resample_long - resample_short\n",
    "\n",
    "# Effect size as mean of test statistic distribution\n",
    "effect_size = np.mean(test_statistic)\n",
    "~~~\n",
    "\n",
    "- Shuffle and split:\n",
    "\n",
    "~~~\n",
    "# Concatenate and Shuffle\n",
    "shuffle_bucket = np.concatenate((group_short,group_long))\n",
    "np.random.shuffle(shuffle_bucket)\n",
    "\n",
    "# Split in the middle\n",
    "slice_index = len(shuffle_bucket)//2\n",
    "shuffled_half1 = shuffle_bucket[:slice_index]\n",
    "shuffled_half2 = shuffle_bucket[slice_index:]\n",
    "~~~\n",
    "\n",
    "- Resample and test again:\n",
    "\n",
    "~~~\n",
    "# Resample shuffled populations\n",
    "shuffled_sample1 = np.random.choice(shuffled_half1,size=500,replace=True)\n",
    "shuffled_sample2 = np.random.choice(shuffled_half2,size=500,replace=True)\n",
    "\n",
    "# Recompute effect size\n",
    "shuffled_test_statistic = shuffled_sample2 - shuffled_sample1\n",
    "effect_size = np.mean(shuffled_test_statistic)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
